{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text classification\n",
    "\n",
    "This week we are moving from  classifiyng characteristics of single words to classifying whole texts. However, instead of trying to classify the sentiment of a text, we will be classifying whether texts are toxic or not. We are using the toxi-text dataset from huggingface. You can find more information about the dataset [here](https://huggingface.co/datasets/FredZhang7/toxi-text-3M). Try to get an overview of:\n",
    "- what kind of data it contains\n",
    "- where the data comes from\n",
    "- what the labels mean\n",
    "\n",
    "If you prefer not to read toxic text you can use [this](https://huggingface.co/datasets/stanfordnlp/imdb) dataset instead which contains imdb reviews and sentiment classification labels - or any other dataset you prefer :-)\n",
    "\n",
    "## Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#in terminal:\n",
    "# pip install nltk pandas numpy gensim scikit-learn fsspec huggingface-hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ucloud/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import gensim.downloader\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "\n",
    "The dataset is very large and multilingual, so for efficiency's sake we will only use a smaller, English subset of the data. We don't have to split the data into training and test sets because the dataset already has a test set which is saved in a separate file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"hf://datasets/FredZhang7/toxi-text-3M/train/multilingual-train-deduplicated.csv\", nrows=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>is_toxic</th>\n",
       "      <th>lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Saved lives, and spent for all of their childr...</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I agree with what you say, but for those worke...</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>My observation is there exists unequal share o...</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Animal based fats are not what causes cardiova...</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@GOPBlackChick @barrackobama just said u.s.was...</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I bet you supported the war on Iraq, or bombin...</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>\"You are seriously comparing pregnancy with th...</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>I like Rachel Notley but  regardless of her in...</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>One's biological sex - male and female - is a ...</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>The irony is delicious. A party that is single...</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  is_toxic lang\n",
       "0   Saved lives, and spent for all of their childr...         0   en\n",
       "1   I agree with what you say, but for those worke...         0   en\n",
       "2   My observation is there exists unequal share o...         0   en\n",
       "3   Animal based fats are not what causes cardiova...         0   en\n",
       "4   @GOPBlackChick @barrackobama just said u.s.was...         0   en\n",
       "5   I bet you supported the war on Iraq, or bombin...         0   en\n",
       "6   \"You are seriously comparing pregnancy with th...         0   en\n",
       "7   I like Rachel Notley but  regardless of her in...         0   en\n",
       "8   One's biological sex - male and female - is a ...         0   en\n",
       "11  The irony is delicious. A party that is single...         0   en"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[df.lang == 'en']\n",
    "df[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "The sklearn bag-of-words model expects the data to be a sequence of strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Saved lives, and spent for all of their children's lives.  \\nLIberal Madness, playing at a theatre near you.\",\n",
       " 'I agree with what you say, but for those workers it must also become expensive to live in Vancouver, so maybe even they would be happier moving slightly further from downtown.  Maybe not as extreme as Toronto...',\n",
       " 'My observation is there exists unequal share of State monies with its residents, before all the Urban residents get defensive please hear me out. Presently no one except Corporations pay State income taxes. No individual pays state taxes. I noticed state funded bicycle paths, road maintenance, defunct Docks, powerful politicians pet projects such as office buildings, state troopers etc, etc. all these fundings and more are not necessary within City limits, I was amazed at how much our state provides city functions in the bigger cities thus growing the state budget, I saw on tv last night how adg&g was showing the little ones how to ice fish, couldn\\'t the parents do that? yes I pay property taxes. But I will guarantee you once our City folk get the full bill with out the State monies we will not be as wasteful, on the office building...rent what space you need and help the office rental marked. In my business I just retired from,I would ask my managers, \"how do you stop spending money\"?',\n",
       " 'Animal based fats are not what causes cardiovascular disease. The research used to pass that false information to patients for decades was misinterpreted and then drug companies covered it up to sell medications that harm health. Of course fats from animals full of hormones and antibiotics or butter from those animals is not healthy, the organic versions are including bacon. We need to eat animal fats to be healthy, especially for our neurological systems. It is the sugar, cigarettes, alcohol and lack of exercise that cause heart disease.',\n",
       " '@GOPBlackChick @barrackobama just said u.s.was full of nice,giving,smart ppl.I thought we were mean,unexceptional,bitter clingers?#evolved?',\n",
       " 'I bet you supported the war on Iraq, or bombing Iran, because, well.....we need to stand up to the \"evil doers\" of the world and show them that their behavior will not be tolerated... but homegrown terrorist....',\n",
       " '\"You are seriously comparing pregnancy with the flu?\"\\n.\\nNo, I am comparing things like Endometriosis, which are prevented or reduced by hormone therapy, to the flu. My post was in reply to a post explicitly responding to \"Many women use this hormone treatment to control their menstrual cycles for reasons beyond birth control.\" You are seriously somehow getting pregnancy prevention from that? Going to such lengths to try to score some sort of rhetorical point against your imagined \"collectivists\" just makes you look bad.',\n",
       " 'I like Rachel Notley but  regardless of her integrity the west sees that she was stabbed in the back by the Federal NDP and Liberals ..the limousine progressives  have totally overplayed their hand and with some help from Coderre the west is pretty much 100% piZZt ... things are tough in towns all across the country\\nand JT comes off like a smug, entitled, trustfunder ... its a bad look',\n",
       " \"One's biological sex - male and female - is a valid distinction. Sexual expression, according to the Catholic Church, is either moral or immoral in its expression.\",\n",
       " 'The irony is delicious. A party that is single handedly ruining the province with increased debt needs to borrow and bribe us to allow them to continue  to put ourselves in debt.\\n\\nLet this be a warning to all.  They have to go.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = df[\"text\"].tolist()\n",
    "texts[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag-of-words \n",
    "\n",
    "One of the simplest way to represent a document is a bag-of-words model. This model represents a document as a set of words, ignoring the order of the words. The model is implemented in the `CountVectorizer` class in sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(86996, 155423)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(stop_words = 'english')\n",
    "features = vectorizer.fit_transform(texts)\n",
    "\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape of the matrix should correspond to the number of documents and the number of unique words in the dataset. The value of each cell should correspond to the number of times the word appears in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#comment out to avoid impossibly long list of key-value pairs to be printed:)\n",
    "#vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "155423\n",
      "86996\n"
     ]
    }
   ],
   "source": [
    "print(len(vectorizer.vocabulary_))\n",
    "print(len(texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we need to create a list of the labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = df.is_toxic.tolist()\n",
    "y[0:10] #print first 10 labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a model\n",
    "\n",
    "Now we can train a model to classify the toxicity of the texts. I will use a simple logistic regression model, but feel free to swap it out for any other model you prefer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 0 (base model)\n",
    "clf = LogisticRegression(random_state=42)\n",
    "clf.fit(features, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(features, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 1 add iterations to the LR\n",
    "clf1 = LogisticRegression(random_state=42, \n",
    "                         max_iter = 1000, \n",
    "                         verbose = True)\n",
    "clf1.fit(features, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9741137523564302"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf1.score(features, y) #clf1 score: 0.9741137523564302"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "removing lowercasing doesnt affect the score \n",
    "stop words = english - improves the score : 0.9741137523564302\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try to take a look at the documentation for the [Countvectorizer](https://scikit-learn.org/1.5/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html). Try to change the parameters of the model and see how it affects the performance of the model:\n",
    "- try to remove lowercasing and see how it affects performance\n",
    "- try to add stopwords to the model\n",
    "- try to see if you can find a parameter that can be used as an alternative to stopword removal\n",
    "- try to change the ngram_range parameter\n",
    "- try to change how the model tokenises the text by changing the token_pattern parameter (hint: use a regex generator)\n",
    "\n",
    "## tf-idf\n",
    "\n",
    "Another simple, yet slightly more advanced model is the tf-idf model. This model is also implemented in sklearn. The model is implemented in the `TfidfVectorizer` class in sklearn.\n",
    "\n",
    "- try to create tfidf features from our texts and run the classifier again\n",
    "- take a look at the [documentation](https://scikit-learn.org/1.5/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) and try to change the parameters of the model and see how it affects the performance of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(stop_words = \"english\") #create vectorizer + incl. stop words\n",
    "features_tfidf = tfidf_vectorizer.fit_transform(texts) #use vectorizer to extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit LogReg model 1 )\n",
    "clf1.fit(features_tfidf, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9331118672122857"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf1.score(features_tfidf, y) \n",
    "# with stop_words = english 0.9331118672122857"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document embeddings\n",
    "\n",
    "A much more nuanced way to represent text is through embeddings. However, most machine learning models require a fixed-size input, so we need to find a way to represent the whole document as a fixed-size vector. One way to do this is to use the average of the word embeddings of the words in the document. We will use the pre-trained word embeddings from the GloVe model. However, using word embeddings requires us to split the documents into individual words. We will use the nltk library to do this, but there are both simpler and more advanced ways to do this. The simplest method would be to split the documents by spaces, while a more advanced method would be to use a tokenizer that is aware of the structure of the language, like the one in the [spacy](https://spacy.io/api/tokenizer) library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we try to tokenise the first of the texts, we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Saved',\n",
       " 'lives',\n",
       " ',',\n",
       " 'and',\n",
       " 'spent',\n",
       " 'for',\n",
       " 'all',\n",
       " 'of',\n",
       " 'their',\n",
       " 'children',\n",
       " \"'s\",\n",
       " 'lives.',\n",
       " 'LIberal',\n",
       " 'Madness',\n",
       " ',',\n",
       " 'playing',\n",
       " 'at',\n",
       " 'a',\n",
       " 'theatre',\n",
       " 'near',\n",
       " 'you',\n",
       " '.']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(texts[0], \n",
    "              language='english', \n",
    "              preserve_line=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can load the embeddings and match our tokenised words to the embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 376.1/376.1MB downloaded\n"
     ]
    }
   ],
   "source": [
    "embeddings = gensim.downloader.load(\"glove-wiki-gigaword-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(text):\n",
    "    return [embeddings[word] for word in word_tokenize(text, language='english', preserve_line=True) if word in embeddings.key_to_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_embeddings = [get_embeddings(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n",
      "300\n"
     ]
    }
   ],
   "source": [
    "print(len(text_embeddings[0]))\n",
    "print(len(text_embeddings[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35\n",
      "300\n"
     ]
    }
   ],
   "source": [
    "print(len(text_embeddings[1]))\n",
    "print(len(text_embeddings[1][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that though the individual word embeddings have to same number of dimensions, the document embeddings have different sizes. We can fix this by taking the average of the word embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_embeddings = [np.mean(embedding, axis=0) for embedding in text_embeddings]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_embeddings[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you have mean document embeddings that you can use to classify the texts!\n",
    "\n",
    "- try to classify the texts using the average of the word embeddings of the words in the text\n",
    "- try lowercasing the words before creating the embeddings\n",
    "- try removing stopwords or punctuation beore creating the embeddings\n",
    "- try using another classifier\n",
    "- try to use all the languages in the dataset and see how it affects the performance of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(random_state=42, \n",
    "                         max_iter = 1000, \n",
    "                         verbose = True)\n",
    "clf.fit(mean_embeddings, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
