{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word embeddings\n",
    "\n",
    "## Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gensim\n",
    "!pip install matplotlib\n",
    "!pip install scikit-learn\n",
    "!pip install transformers\n",
    "!pip install torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last week, we talked about how ```Hugging Face``` is the place to go for pretrained models. Today, we're going to meet ```gensim``` which is a library for working with (static) word embeddings like word2vec. You can find the documentation [here](https://radimrehurek.com/gensim/). Additionally, ```scikit-learn``` provides a whole host of fundamental machine learning algortithms and tools. You can find the documentation [here](https://scikit-learn.org/stable/).\n",
    "\n",
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[```GloVe```](https://nlp.stanford.edu/projects/glove/) word vectors are trained on aggregated global word-word co-occurrence statistics from a corpus. ```glove-wiki-gigaword-300``` are 300-dimensional vectors trained on co-occurence statistics from Wikipedia articles and various news outlets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = gensim.downloader.load(\"glove-wiki-gigaword-300\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've outlined a couple of tasks for you below to experiment with. Use these as a starting point to explore the nature of word embeddings and how they work.\n",
    "\n",
    "Work in small groups on these tasks and make sure to discuss the issues and compare results.\n",
    "\n",
    "## Task 1 - Synonyms & antonyms\n",
    "\n",
    "As introduced in the lecture, we can use _cosine similarity_ to quantify the similarity between words embeddings, since words with similar meanings tend to have vectors pointing in similar directions, resulting in a higher cosine similarity. We can leverage this relationship for various tasks in NLP, such as finding synonyms and antonyms.\n",
    "\n",
    "_Cosine similarity_ can also be thought of as _cosine distance_, which is simply ```1 - cosine similarity```. So the higher the cosine distance, the further away two words are from each other and so they have less \"in common\". Consequently, we would expect synonyms to have a lower cosine distance than antonyms.\n",
    "\n",
    "You can use the the ```embeddings.distance()``` function to compute the cosine distance between two words and the ```embeddings.most_similar()``` function to examine what other words are closest to a target word. Here is a small example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings.most_similar(\"doctor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings.distance(\"doctor\", \"physician\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to find three words ```(w1,w2,w3)``` where ```w1``` and ```w2``` are synonyms and ```w1``` and ```w3``` are antonyms, but where:\n",
    "\n",
    "```Cosine Distance(w1,w3) < Cosine Distance(w1,w2)```\n",
    "\n",
    "For example, w1=\\\"happy\\\" is closer to w3=\\\"sad\\\" than to w2=\\\"cheerful\\\".\n",
    "\n",
    "I've given a starting example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings.distance(\"happy\", \"sad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings.distance(\"happy\", \"cheerful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings.distance(\"happy\", \"sad\") < embeddings.distance(\"happy\", \"cheerful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have found your example,\n",
    "- __give a possible explanation for why this counter-intuitive result may have happened.__\n",
    "\n",
    "## Task 2 - Analogies\n",
    "\n",
    "We saw in the lecture that we can use basic arithmetic on word embeddings, in order to conduct a word analogy task.\n",
    "\n",
    "For example:\n",
    "\n",
    "```king - man + woman = queen```\n",
    "\n",
    "If we take the vector for ```king``` and subtract the vector for ```man```, we're removing the gender component from the ```king```. If we then add ```woman``` to the resulting vector, we should be left with a vector similar to ```queen```.\n",
    "\n",
    "NB: It might not be _exactly_ the vector for ```queen```, but it should at least be _close_ to it.\n",
    "\n",
    "```gensim``` has some quirky syntax that allows us to perform this kind of arithmetic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings.most_similar(positive=['king', 'woman'], \n",
    "                   negative=['man'])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to\n",
    "- __find at least three analogies which correctly hold - where \"correctly\" means that the closest vector corresponds to the word that you expect it would.__\n",
    "\n",
    "## Task 3 - Wrong analogies\n",
    "\n",
    "Can you \n",
    "- __find any analogies which should hold but don't?__\n",
    "- __Why don't they work? Are there any similarities or trends?__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings.most_similar(positive=['', ''], \n",
    "                   negative=[''])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4 - Bias\n",
    "\n",
    "As we spoke briefly about in the lecture, word embeddings tend to display bias of the kind found in the training data.\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings.most_similar(positive=['nurse', 'man'], \n",
    "                   negative=['woman'])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using some of the techniques you've worked on above, \n",
    "- __can you find some clear instances of bias in the word embeddings?__\n",
    "\n",
    "## Task 5 - LLMs\n",
    "\n",
    "Try to load in the language model from last week and figure out how best to prompt it to generate analogies.\n",
    "\n",
    "- __How does it compare to the word embeddings?__ \n",
    "- __What are the advantages and disadvantages of each approach?__ \n",
    "- __Does it seem to be biased in the same way as the word embeddings?__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import transformers \n",
    "import torch\n",
    "\n",
    "model = \"google/flan-t5-base\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=model,\n",
    "    torch_dtype=torch.float16,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6 - Visualisation\n",
    "\n",
    "In the following cell, I've written a short bit of code which takes a given subset of words and plots them on a simple scatter plot. Remember that the word embeddings are 300D, so we need to perform some kind of dimensionality reduction on the embeddings to get them down to 2D.\n",
    "\n",
    "Here, I'm using a simple PCA algorithm implemented via ```scikit-learn```. An alternative approach might also be to use Singular Value Decomposition or SVD, which works in a similar but ever-so-slightly different way to PCA. You can read more [here](https://jeremykun.com/2016/04/18/singular-value-decomposition-part-1-perspectives-on-linear-algebra/) and [here](https://jonathan-hui.medium.com/machine-learning-singular-value-decomposition-svd-principal-component-analysis-pca-1d45e885e491).\n",
    "\n",
    "Experiment with plotting certain subsets of words by changing the ```words``` list.\n",
    "\n",
    "- __How useful do you find these plots?__\n",
    "- __Do they show anything meaningful?__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the list of words we want to plot\n",
    "words = [\"man\", \"woman\", \"doctor\", \"nurse\", \"king\", \"queen\", \"boy\", \"girl\"]\n",
    "\n",
    "# an empty list for vectors\n",
    "X = []\n",
    "# get vectors for subset of words\n",
    "for word in words:\n",
    "    X.append(embeddings[word])\n",
    "\n",
    "# Use PCA for dimensionality reduction to 2D\n",
    "pca = PCA(n_components=2)\n",
    "result = pca.fit_transform(X)\n",
    "\n",
    "# or try SVD - how are they different?\n",
    "# svd = TruncatedSVD(n_components=2)\n",
    "# fit_transform the initialized PCA model\n",
    "# result = svd.fit_transform(X)\n",
    "\n",
    "# create a scatter plot of the projection\n",
    "plt.scatter(result[:, 0], result[:, 1])\n",
    "\n",
    "# for each word in the list of words\n",
    "for i, word in enumerate(words):\n",
    "    plt.annotate(word, xy=(result[i, 0], result[i, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus tasks\n",
    "\n",
    "If you run out of things to explore with these embeddings, try some of the following tasks:\n",
    "\n",
    "- __make new plots like those above but cleaner and more informative__\n",
    "- __write a script which takes a list of words and produces the output above__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
